{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and its usage in Categorizing Exoplanets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1: Introduction to Neural Networks\n",
    "\n",
    "### Neural Networks Vocabulary\n",
    "**Neural Networks** is a computer system used in deep learning. They utilize an input-output process where the inputs are subject to functions and weights to determine *hidden units* and the output reflects as such. The concept of Neural Networks is inspired by the processes within the human brain and neurons firing to indicate some input-output.\n",
    "\n",
    "**Hidden Units** are units which we find in each layer of the Neural Network. All inputs of our Neural Network are connected to these hidden values, though with differing dependencies which are denoted as *weights*.\n",
    "\n",
    "**Weights** in a Neural Network help us to denote the dependency of our hidden values to our inputs. For example, say one of our Hidden Units is a host star's type. Inputs such as the star's temperature and radius will have higher weights in determining the host star's type. The orbital distance of an orbiting exoplanet, however, will likely have a much lower weight, as the orbital distance of this exoplanet will not explicitly help us to identify its host star's type.\n",
    "\n",
    "![simplified graphic of three layers in a neural network](Resources/weights-image.webp)\n",
    "\n",
    "**Layers** in a Neural Network consist of a couple parts each: the first layer is some singular or a vector of inputs, whether they be the initial inputs or outputs from the last layer. Next, the inputs are weighed, as discussed above. Afterwards, the data is transformed by some functions which will be discussed further later on. Lastly, the layer has some output which either goes to an *activation function* at the end of the Neural Network, or the inputs are the final output. \n",
    "\n",
    "![simplified graphic of three layers in a neural network](Resources/neural_network_w_matrices.png)\n",
    "\n",
    "An **Activation Function** is the final portion of the Neural Network which decides whether the final output of the \"neuron\" fires; for a binary classification, for example, this portion makes the decision of whether it is a 1 or a 0, depending on the criteria decided by the inputs and hidden units.\n",
    "\n",
    "---\n",
    "### Homework: Perceptron\n",
    "In basic terms, a **Perceptron** is a single layer Neural Network, meaning it contains all parts of a singular layer listed above. \n",
    "\n",
    "![simplified graphic of a perceptron](Resources/perceptron-image.webp)\n",
    "\n",
    "Perceptrons are usually used for Binary classification, or classifying the data into two parts.\n",
    "\n",
    "---\n",
    "### References\n",
    "*Concepts*. ML Glossary. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html\n",
    "\n",
    "Sharma, Shagar. (2017, September 9). *What the Hell is Perceptron?: The Fundamentals of Neural Networks.* Towards Data Science. https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2: Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Vocabulary and Understanding\n",
    "\n",
    "**Logistic Regression** is used in binary classification and can be utilized to create a model for predicting these binary outcomes based on similar precedented variables. To be able to make accurate predictions, these models requires *training sets*.\n",
    "\n",
    "**Training Sets** are small matrix sets of data with known outputs. By using these in our models, we can create progressively more accurate predictions for outputs of our data given some precedented or similar inputs. \n",
    "\n",
    "A **Linear Regression Model** is a regression model used to predict trends in data given some input, but which assumes the relationship between input and output is linear. In this relationship, the weighted value of the input is the slope and has some constant b as the y-intercept. However, given a model with binary classification, we will want our y-values to be between or equal to 0 and 1. To do this, we will require a *Sigmoid function*.\n",
    "\n",
    "A **Sigmoid Function** allows us to make the output a probability as opposed to the literal target variable; this allows us to make binary classifications of our data; for example, rather than having an output pertaining to the literal size of an exoplanet, the Sigmoid Function would transform the data to simply tell us whether or not the size of the planet is *yes*, big enough to be a Hot Jupiter, or *No*, not big enough to be a Hot Jupiter (of course other factors go into making this decision, but let's keep things simple).\n",
    "\n",
    "To apply the Sigmoid Function, we simply make our linear function the input of the Sigmoid Function, and thus it becomes our *Activation function*.\n",
    "\n",
    "---\n",
    "### Homework: \n",
    "\n",
    "Consider *n* inputs and *m* data in our training set. Find the dimension of *X*, *X<sup>i</sup>*, *w*, *b*, *z*, and *a* where *z* is the input of the Sigmoid function and *a* is the Activation function.\n",
    "\n",
    "*X* = [(x<sup>1</sup>)<sub>1</sub>, (x<sup>2</sup>)<sub>2</sub>,...,(x<sup>n</sup>)<sub>n</sub>]\n",
    "\n",
    "*X<sup>i</sup>* = [*X*]\n",
    "\n",
    "*w* = weighted value\n",
    "\n",
    "*b* = y-intercept\n",
    "\n",
    "*z* = *wX<sup>i</sup>* + *b*\n",
    "\n",
    "*a* = 1/(1 + e<sup>-(*wX<sup>i</sup>* + *b*)</sup>)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 2 Variable Cheatsheet\n",
    "**Used in either the lecture or the notes**\n",
    "| Variable | Meaning |\n",
    "| -------- | ------- |\n",
    "| *y* | output ε {0, 1} |\n",
    "| *Y* | output matrix of our training set |\n",
    "| *y<sup>i</sup>* | output to corresponding input *X<sup>i</sup>* |\n",
    "| *x* | input vector |\n",
    "| *X<sup>i</sup>* | Each column in the input |\n",
    "| *n* | length of input vector |\n",
    "| *m* | number of inputs for which we have a known correct output |\n",
    "| *z* | the linear relationship between the output and input where *z* = *wX<sup>i</sup>+b* |\n",
    "| *w* | weight of input |\n",
    "| *b* | constant bias |\n",
    "| *σ*(*z*) | sigmoid function where *σ*(*z*) = 1/(1+e<sup>*-z*</sup>) |\n",
    "| *a* | activation function where *a* = *σ*(*wX* + *b*) |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3: Loss and Cost Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Cost Functions Vocabulary and Understanding\n",
    "\n",
    "A **Loss/Error Function** is used to measure how well a model can make predictions. We say we want to \"minimize\" our Loss Function, meaning we want to find the parameters necessary for our model to make the best possible predictions.\n",
    "\n",
    "There are multiple common Loss Functions we can utilize, but for now we will focus on the **Log-Likelihood Loss Function** which states \n",
    "*L(a,y<sup>i</sup>) = -y<sup>i</sup>log(a) - (1 - y<sup>i</sup>)log(1 - a)* where *a* is the predicted output and *y<sup>i</sup>* is the known output. To minimize the loss function would mean that the overall difference between the actual and predicted outputs is as small as possible. \n",
    "\n",
    "A **Cost Function** gives an idea of the average loss from the Loss Function for the entire training set involved. We define the Cost Function as\n",
    "*J(w,b) = 1/m * (The sum from 1 to m of L(a, y<sup>i</sup>))* where *w* is the weighted value, *b* is bias, and *m* is the number of input-output sets we have in our training set.\n",
    "\n",
    "### Homework:\n",
    "\n",
    "A necessary and sufficient condition for a function *f(x)* to be convex on a interval is that the second derivative *d<sup>2</sup>f/dx<sup>2</sup>* >= 0 for all x in the interval. We can show a local minimum of a convex function is also a global minimum. In a neural network we can try to minimize the cost function. Does the cost function have to be convex?\n",
    "\n",
    "The cost function does not necessarily have to be convex, however if it is convex it will allow us to minimize our cost function to the best possible ability. This is because convex functions have a global minimum, allowing us to have a best overall possible minimum to our cost function, which would be the goal. However, given a non-convex function, we will have multiple local minima but no global minimum which would mean we would be unable to optimize the minimization of our cost function.\n",
    "\n",
    "---\n",
    "### References\n",
    "Mack, Conor. (2017, November 27). *Machine Learning Fundamentals (I): Cost Functions and Gradient Descent*. Towards Data Science. https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220#:~:text=Put%20simply%2C%20a%20cost%20function,to%20as%20loss%20or%20error.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3 Variable Cheatsheet\n",
    "**Used in either the lecture or the notes**\n",
    "\n",
    "| Variable | Meaning |\n",
    "| -------- | ------- |\n",
    "| *L*(*a*, *y<sup>i</sup>*) | Log-Likelihood Function|\n",
    "| *a* | Predicted output |\n",
    "| *y<sup>i</sup>* | Known output |\n",
    "| *J*(*w*, *b*) | Cost Function |\n",
    "| *w* | Weight of input |\n",
    "| *b* | Constant bias |\n",
    "| *m* | Number of elements in the training set |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 4: Gradient Descent\n",
    "### Gradient Descent Vocabulary\n",
    "**Gradient Descent** is a minimization algorithm oftem utilized in deep learning. It is used for optimization and for training neural networks. Using Gradient Descent, we are able to train the data to have the correct weight and bias values and minimize the cost function.\n",
    "\n",
    "Gradient Descent requires a **Learning Scale** to function, or the step size. The Learning Scale is the size of the steps taken to reach that minimum value. The Learning Scale should be reasonable, not too large or too small. If it is too large, our function runs the risk of overshooting the minimum. On the other hand, if it is too small, it may be more precise but it will also be less efficient. A small (not excessively small) value is best for the Learning Scale.\n",
    "\n",
    "The steps taken to determine the weight and bias using gradient descent are as follows:\n",
    "1. Weight and Bias are initially defined, randomly or by some guess,\n",
    "2. We find the slope of the cost function,\n",
    "3. We adjust the Weight and Bias values by checking along the nearest steepest descent in reference to the slope and changing to the next set of weight and bias values,\n",
    "4. Repeat until the Weight and Bias values do not have any significant change, therefore indicating we have reached the minimum of the cost function.\n",
    "\n",
    "This process can be mathematically defined as:\n",
    "*w* := *w* - *α(dJ(w, b)/dw)* and *b* := *b* - *α(dJ(w, b)/db)* \n",
    "\n",
    "where *α* is the learning scale.\n",
    "\n",
    "The **Backpropogation Method** is the way by which we find the derivative (slope of the tangent line) of the loss function with respect to the weights of our given inputs and the bias. This method finds the derivative with respect to multiple instances: first with respect to *z*, where *z*=*wX<sup>i</sup>* + *b*, then to *y*, where *y* is the output for a given input, then top *w<sub>1</sub>*, *w<sub>2</sub>*, and *b*. After deriving these final values, we are able to modify the values as mathematically defined.\n",
    "\n",
    "After modifying our values, we repeat this process as described in step 4 above.\n",
    "\n",
    "### Homework:\n",
    "In programming, the term *dL/dq* is also denoted by *dq* where *q* is a parameter, such as *z* or *w<sub>i</sub>*. Calculate *da*, *dz*, *dw<sub>1</sub>*, *dw<sub>2</sub>*, and *db* using the mathematical definition of a loss function and considering sigmoid function.\n",
    "\n",
    "*da* = -*y<sup>i</sup>*log(*a*)-(1-*y<sup>i</sup>*)log(1-*a*)*dL/da* = -(*y<sup>i</sup>*/*a*ln(10))-((1-*y<sup>i</sup>*)/(ln(10)-*a*ln(10)))\n",
    "\n",
    "*dz* = *y<sup>i</sup>*log(1/(1+*e<sup>-z</sup>*))-(1-*y<sup>i</sup>*)log(1-(1/(1+*e<sup>-z</sup>*)))*dL/dz* = ((-*y<sup>i</sup>e<sup>-z</sup>*)/(*e<sup>-z</sup>*ln(10)+ln(10)))+(((1-*y<sup>i</sup>*)*e<sup>-z</sup>*)/(*e<sup>-z</sup>*ln(10)+**e<sup>-2z</sup>*ln(10)))\n",
    "\n",
    "*dw<sub>1</sub>* = *y<sup>i</sup>*log(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>))-(1-*y<sup>i</sup>*)log(1-(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>))*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>*dL/dw<sub>1</sub>* = -(*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)/(ln(10)+ln(10)*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)\n",
    "\n",
    "*dw<sub>2</sub>* = *y<sup>i</sup>*log(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>))-(1-*y<sup>i</sup>*)log(1-(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>))*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>*dL/dw<sub>2</sub>* = -(*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)/(ln(10)+ln(10)*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)\n",
    "\n",
    "*db* = *y<sup>i</sup>*log(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>))-(1-*y<sup>i</sup>*)log(1-(1/(1+*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>))*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*))</sup>*dL/db* = -(*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)/(ln(10)+ln(10)*e*<sup>-*w<sub>1</sub><sup>i</sup>x<sub>1</sub><sup>i</sup>*+*w<sub>2</sub><sup>i</sup>x<sub>2</sub><sup>i</sup>*+*b*</sup>)\n",
    "\n",
    "### References\n",
    "*What is Gradiant Descent?*. IBM. https://www.ibm.com/topics/gradient-descent\n",
    "\n",
    "*Weights and Biases*. AI Wiki. https://machine-learning.paperspace.com/wiki/weights-and-biases\n",
    "\n",
    "Gosavi, Bhushan. (2019 November 4). *Mathematics Behind the Artificial Neural Networks: Part 1*. Medium. https://medium.com/analytics-vidhya/mathematics-behind-artificial-neural-networks-part-1-2214dab225c2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 4 Variable Cheatsheet\n",
    "**Used in either the lecture or the notes**\n",
    "| Variable | Meaning |\n",
    "| -------- | ------- |\n",
    "| *w* | Weight parameter |\n",
    "| *b* | Constant bias parameter |\n",
    "| *α* | Learning scale | \n",
    "| *x<sub>n</sub>* | some input number n |\n",
    "| *z* | the linear relationship between the output and input where *z* = *wX<sup>i</sup>+b* |\n",
    "| *a* | activation function |\n",
    "| *σ*(*z*) | sigmoid function where *σ*(*z*) = 1/(1+e<sup>*-z*</sup>) |\n",
    "| *y<sup>i</sup>* | output to corresponding input *X<sup>i</sup>* |\n",
    "| *X<sup>i</sup>* | Each column in the input |\n",
    "| *d* | derivative |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 5: Gradient Descent over All Elements in Training Set\n",
    "### Notes\n",
    "As discussed in Lecture 4, the following is the equation used to modify the weight for Gradient Descent:\n",
    "\n",
    "δ/δ*w*<sub>1</sub> J(*w*,*b*) = (1/*m*)Σ<sub>i=1</sub><sup>m</sup>δ/δ*w*<sup>i</sup><sub>1</sub>*L*(a<sup>i</sup>,y<sup>i</sup>)\n",
    "\n",
    "*Remember!* in Neural Networks we need to avoid **for-loops**. Programs like this are called **vectorized code**. This is necessary for the speed of our program; when we are training our neural network using Gradient Descent, multiple for loops will only slow down our algorithm, especially since we are already spending a decent bit of time on this algorithm.\n",
    "\n",
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.5237431, 0.0438542]]), 0.47648110595188886)\n"
     ]
    }
   ],
   "source": [
    "# Given program\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# returns our sigmoid function σ(x)\n",
    "# math.exp(): returns E to the power of x \n",
    "def sigmoid(x):\n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "# returns activation function a\n",
    "# np.exp(): returns E to the power of x for all elements in array x\n",
    "# ** operator: exponent\n",
    "def sigmoid_prim(x):\n",
    "    a = (np.exp(-x))/(1 + np.exp(-x))**2\n",
    "    return a\n",
    "\n",
    "# gradient descent function given one node j and one layer l\n",
    "def gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate):\n",
    "    alpha = learning_rate\n",
    "\n",
    "    # continuously returns the next weight and bias parameter until we reach the accepted minimum cost weight and bias\n",
    "    for k in range(n_iteration): # for all k < n_iteration\n",
    "        z = 0; db = 0; J = 0; # initialize relationship of input and output as 0, bias as 0\n",
    "        n = X_t.shape[1] # .shape: used to get current shape of an array where, given dimensions [n, m], .shape[0] returns n\n",
    "        d_omega = np.zeros((1, n)) #np.zeros: create a new array of given shapes and types filled with zero values\n",
    "                                   #in this case, we've created a 2D array [1][n] of zeros for our derived weight parameter\n",
    "\n",
    "        omega = np.random.rand(1, n) * np.sqrt(1/n) # initial values for [omega_1, omega_2], our weight parameters\n",
    "        b = np.random.rand() # initial value for bias parameter\n",
    "\n",
    "        m = len(Y) # size of output array Y: number of exoplanets\n",
    "\n",
    "        # takes our initialized variables and uses them to calculate the linear relationship of our inputs and outputs, the activation function given z, and the cost of our current outputs given current parameters\n",
    "        # looping over all planets\n",
    "        for i in range (1, m, +1): # iterable i < m, i++, iterating though array Y\n",
    "            z = np.dot(omega, X_t[i]) + b # np.dot: returns the dot product of the given arrays\n",
    "                                          # in this case calculating the linear relationahip using the weights array, the inputs, and the bias parameter\n",
    "            a = sigmoid(z) #activating function z with sigmoid\n",
    "            \n",
    "            J += -Y[i] * np.log(a) - (1 - Y[i]) * np.log(1-a) # calculating our cost\n",
    "            dz = a - Y[i]\n",
    "\n",
    "            # calculates the next weight parameter by taking a fraction of the derivative, multiplying this by the learning rate, and subtracting from our weight the new rate\n",
    "            # looping over all predictors for each planet\n",
    "            for j in range(0, n - 1): # for iterable j < [0] dimension of input array\n",
    "                d_omega[j] = d_omega[j] / m\n",
    "                omega[j] = omega[j] - alpha * d_omega[j]\n",
    "            db = db / m # calculating the derivative of the bias\n",
    "            b = b - alpha * db # calculating the next bias parameter\n",
    "\n",
    "        return(omega, b)\n",
    "\n",
    "def main():\n",
    "    # add more than one dimension to array and transpose it and see if .shape can pull out integer\n",
    "    X_t = (np.array([[5, 15, 25], [5, 15, 25]])).T\n",
    "    Y = np.array([5, 20, 14])\n",
    "    n_iteration = 100_000\n",
    "    learning_rate = 0.0008\n",
    "\n",
    "    #print(X_t)\n",
    "    #print(X_t.shape[0])\n",
    "    #print(type(X_t.shape[0]))\n",
    "    print(gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[39mprint\u001b[39m(gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[22], line 68\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m n_iteration \u001b[39m=\u001b[39m \u001b[39m100_000\u001b[39m\n\u001b[1;32m     66\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0008\u001b[39m\n\u001b[0;32m---> 68\u001b[0m \u001b[39mprint\u001b[39m(gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n",
      "Cell \u001b[0;32mIn[22], line 58\u001b[0m, in \u001b[0;36mgradient_descent_one_node_one_layer\u001b[0;34m(X_t, Y, n_iteration, learning_rate)\u001b[0m\n\u001b[1;32m     55\u001b[0m     b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m alpha \u001b[39m*\u001b[39m db\n\u001b[1;32m     57\u001b[0m     k \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 58\u001b[0m \u001b[39mmap\u001b[39m(gdParameters(k), k \u001b[39m<\u001b[39m n_iteration)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m(omega, b)\n",
      "Cell \u001b[0;32mIn[22], line 28\u001b[0m, in \u001b[0;36mgradient_descent_one_node_one_layer.<locals>.gdParameters\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     25\u001b[0m n \u001b[39m=\u001b[39m X_t\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m d_omega \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, n))\n\u001b[0;32m---> 28\u001b[0m omega \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrand((\u001b[39m1\u001b[39;49m, n)) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mn)\n\u001b[1;32m     29\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand()\n\u001b[1;32m     31\u001b[0m m \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(Y)\n",
      "File \u001b[0;32mmtrand.pyx:1202\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.rand\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:436\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.random_sample\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:307\u001b[0m, in \u001b[0;36mnumpy.random._common.double_fill\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# my vectorized solution\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# returns our sigmoid function σ(x)\n",
    "# math.exp(): returns E to the power of x \n",
    "def sigmoid(x):\n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "# returns activation function a\n",
    "# np.exp(): returns E to the power of x for all elements in array x\n",
    "# ** operator: exponent\n",
    "def sigmoid_prim(x):\n",
    "    a = (np.exp(-x))/(1 + np.exp(-x))**2\n",
    "    return a\n",
    "\n",
    "def gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate):\n",
    "    alpha = learning_rate\n",
    "    \n",
    "    omega = 0;\n",
    "    b = 0;\n",
    "    k = 0; i = 1; j = 0; h = 0; #iterables\n",
    "    def gdParameters(k):\n",
    "        z = 0; db = 0;\n",
    "        n = X_t.shape[0]\n",
    "        d_omega = np.zeros((1, n))\n",
    "\n",
    "        omega = np.random.rand((1, n)) * np.sqrt(1/n)\n",
    "        b = np.random.rand()\n",
    "\n",
    "        m = len(Y)\n",
    "\n",
    "        def activate_and_calcCost(i):\n",
    "            z = np.dot(omega, X_t[i]) + b\n",
    "            a = sigmoid(z)\n",
    "\n",
    "            J += Y[i] * np.log(a) - (1 - Y[i]) * np.log(1 - a)\n",
    "            dz = a - Y[i]\n",
    "\n",
    "            def omegaDerivative(j):\n",
    "                d_omega[j] += X_t[i][j] * dz\n",
    "                j += 1\n",
    "            map(omegaDerivative(j), j < n)\n",
    "\n",
    "            i += 1\n",
    "        map(activate_and_calcCost(i), i < m)\n",
    "\n",
    "        J = J/m\n",
    "        def calcNextOmega(j):\n",
    "            d_omega[j] = d_omega[j] / m\n",
    "            omega[j] = omega[j] - alpha * d_omega[j]\n",
    "        map(calcNextOmega(j), j < n)\n",
    "\n",
    "        db = db/m\n",
    "        b = b - alpha * db\n",
    "\n",
    "        k += 1\n",
    "    map(gdParameters(k), k < n_iteration)\n",
    "\n",
    "    return(omega, b)\n",
    "\n",
    "def main():\n",
    "    X_t = np.array([5, 15, 25, 35, 45, 55])\n",
    "    Y = np.array([5, 20, 14, 32, 22, 38])\n",
    "    n_iteration = 100_000\n",
    "    learning_rate = 0.0008\n",
    "\n",
    "    print(gradient_descent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,2) (3,) (1,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[33], line 49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m n_iteration \u001b[39m=\u001b[39m \u001b[39m100_000\u001b[39m\n\u001b[1;32m     47\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0008\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[39mprint\u001b[39m(gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n",
      "Cell \u001b[0;32mIn[33], line 38\u001b[0m, in \u001b[0;36mgradient_decent_one_node_one_layer\u001b[0;34m(X_t, Y, n_iteration, learning_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m     db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dZ, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m/\u001b[39mm\n\u001b[1;32m     37\u001b[0m     test \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m d_omega\n\u001b[0;32m---> 38\u001b[0m     omega \u001b[39m-\u001b[39;49m\u001b[39m=\u001b[39;49m test\n\u001b[1;32m     39\u001b[0m     b \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m db\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m(omega, b)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,2) (3,) (1,2) "
     ]
    }
   ],
   "source": [
    "#Actual Solution\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "  \n",
    "    return(1 / (1 + math.exp(-x))) \n",
    "\n",
    "\n",
    "def gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learning_rate):\n",
    "    \n",
    "    alpha = learning_rate         \n",
    "    \n",
    "    for k in range(n_iteration):\n",
    "        \n",
    "        z = 0; db = 0 \n",
    "        n = 2\n",
    "        #n = X_t.shape[0] \n",
    "        d_omega = np.zeros((1, n)) \n",
    "          \n",
    "        rng = np.random.default_rng()\n",
    "        omega = rng.random((1,n))* np.sqrt(1/n)\n",
    "        b = np.random.rand(1, 1)\n",
    "                                 \n",
    "        i = 0\n",
    "        Z = np.dot(omega , X_t[i]) + b\n",
    "        A = sigmoid(Z)\n",
    "        dZ = A - Y\n",
    "        m = len(Y)\n",
    "        \n",
    "        J = -  Y * np.log(A) - (1 - Y) * np.log(1 - A) \n",
    "        # cost = np.sum(J, axis=1)/m\n",
    "        d_omega = (1/m) * np.dot(dZ, X_t.T)\n",
    "        db = np.sum(dZ, axis=0)/m\n",
    "\n",
    "        omega -= alpha * d_omega\n",
    "        b -= alpha * db\n",
    "    \n",
    "    return(omega, b) \n",
    "\n",
    "def main():\n",
    "    X_t = (np.array([[2,2,6],[1,2,88]])).transpose()\n",
    "    Y = np.array([5, 20])\n",
    "    n_iteration = 100_000\n",
    "    learning_rate = 0.0008\n",
    "\n",
    "    print(gradient_decent_one_node_one_layer(X_t, Y, n_iteration, learning_rate))\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 5 Variable Cheatsheet\n",
    "**Used in either the lecture or the notes**\n",
    "| Variable | Meaning |\n",
    "| -------- | ------- |\n",
    "| *δ* or *d* | derivative |\n",
    "| *a* | activation function |\n",
    "| *σ(x)* | Sigmoid function |\n",
    "| *α* or alpha | learning rate |\n",
    "| *k* | iterable |\n",
    "| *X_t* | input array *X<sup>i</sup>* where *t* is the transpose of the input matrix (?) |\n",
    "| *Y* | ouput array |\n",
    "| n_iteration | number of nodes to iterate through |\n",
    "| *z* | the linear relationship between the input and output in the training set |\n",
    "| *db* | the derivative of the bias parameter |\n",
    "| d_omega | the derivative of the weight parameter |\n",
    "| *n* | [0] dimension of the input array |\n",
    "| omega | initial values for [omega_1, omega_2], our weight parameters |\n",
    "| *b* | initial value for bias parameter |\n",
    "| *m* | size of output array *Y* |\n",
    "| *i* | iterable |\n",
    "| *J* | Cost function |\n",
    "| *dz* | Difference between predicted output and known output |\n",
    "| *j* | iterable |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Bakhvalov, Denis. (10 November 2017). *Vectorization Part 7. Tips for Writing Vectorizable Code*. Easyperf. https://easyperf.net/blog/2017/11/10/Tips_for_writing_vectorizable_code\n",
    "\n",
    "*Built-In Functions*. Python Documentation. https://docs.python.org/3/library/functions.html#map\n",
    "\n",
    "Hsu, Jonathan. (2019 December 14). *How to Replace your Python For Loops with Map, Filter, and Reduce*. Better Programming. https://betterprogramming.pub/how-to-replace-your-python-for-loops-with-map-filter-and-reduce-c1b5fa96f43a\n",
    "\n",
    "Malli. *Python NumPy zeros() Function*. SparkBy{Examples}. https://sparkbyexamples.com/numpy/numpy-zeros-function/#:~:text=NumPy%20zeros()%20function%20is,of%20floating%20values%20by%20default. \n",
    "\n",
    "Pozo Ramos, Leodanis. *Python's map(): Processing Iterables Without a Loop*. Real Python. https://realpython.com/python-map-function/\n",
    "\n",
    "*Python: What is the difference between math.exp and numpy.exp and why do numpy creators choose to introduce exp again*. Stack Overflow. https://stackoverflow.com/questions/30712402/python-what-is-the-difference-between-math-exp-and-numpy-exp-and-why-do-numpy-c \n",
    "\n",
    "Shah, Ahmar. (2022 Jul 22). *Understand Vectorization for Deep Learning*. Medium. https://towardsdatascience.com/understand-vectorization-for-deep-learning-d712d260ab0f\n",
    "\n",
    "Tarleton, Shane. (2022 February 23). *Use map() instead of for() loops*. Medium. https://blog.bitsrc.io/please-use-map-instead-of-for-loops-5a2f54f088c8\n",
    "\n",
    "Waqar, Hassaan. (2023). *What is the exp function in NumPy?*. Educative. https://www.educative.io/answers/what-is-the-exp-function-in-numpy \n",
    "\n",
    "*What does .shape[] do in \"for i in range(Y.shape[0])\"?* Stack Overflow. https://stackoverflow.com/questions/10200268/what-does-shape-do-in-for-i-in-rangey-shape0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 6: Deep Neural Network\n",
    "### Deep Neural Network Vocabulary\n",
    "There are two parts to involved in hidden layers:\n",
    "1. We calculate *z*=*w<sup>T</sup>X*+*B*, or the output depending on our input and its weights and bias.\n",
    "2. We calculate our activation function *a*=*g(z)*\n",
    "\n",
    "![simplified graphic of a neural network including equations](Resources/neural%20network%20diagram.jpeg)\n",
    "\n",
    "We know we can have multiple layers within our neural network. These layers can differ in multiple ways:\n",
    "- They can each have differing numbers of inputs\n",
    "- The amount of **nodes** in each layer can vary: **Nodes** are the units that have one of more inputs, an activation function, and an output.\n",
    "\n",
    "A **Deep Neural Network** is made by incorporating more nodes and layers. The two parts to computing nodes in each layers has the general form:\n",
    "\n",
    "1. *z<sub>j</sub><sup>[l]</sup>* = (*w<sup>T</sup>*)*<sub>j</sub><sup>[l]</sup>a<sup>[l-1]</sup>*+*b<sub>j</sub><sup>[l]</sup>*\n",
    "\n",
    "2. *a<sub>j</sub><sup>[l]</sup>* = *g<sup>[l]</sup>*(*z<sub>j</sub><sup>[l]</sup>*)\n",
    "\n",
    "where *a<sup>[0]</sup>* represents the input layer, *l* represents the number of layers, *j* is the node's index in the given layer, and *a<sup>l</sup>* is the output of each layer.\n",
    "\n",
    "We can define the output layers as:\n",
    "\n",
    "*Z<sup>[l]</sup>* = [ *z<sup>[l]</sup><sub>1</sub>*, *z<sup>[l]</sup><sub>2</sub>*, ... , *z<sup>[l]</sup><sub>n<sub>1</sub></sub>* ]\n",
    "\n",
    "*A<sup>[l]</sup>* = [ *a<sup>[l]</sup><sub>1</sub>*, *a<sup>[l]</sup><sub>2</sub>*, ... , *a<sup>[l]</sup><sub>n<sub>1</sub></sub>* ]\n",
    "\n",
    "where *n<sub>1</sub>* is the number of nodes in the layer *l*.\n",
    "\n",
    "### Homework\n",
    "Consider a deep neural network with L layers and n inputs. Eahc layer has *n<sub>1</sub>* nodes. Find the rank of the matrices *w<sup>[1]</sup><sub>j</sub>*, *b<sup>[1]</sup><sub>j</sub>Z<sup>[1]</sup>*, *A<sup>[1]</sup>*, *w<sup>[1]</sup>*, *b<sup>[1]</sup>*, *Z<sup>[1]</sup>*, *A<sup>[1]</sup>*.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 6 Variable Cheatsheet\n",
    "**Used in either the lecture or the notes**\n",
    "| Variable | Meaning |\n",
    "| -------- | ------- |\n",
    "| *w* | Weight parameter |\n",
    "|*T* | the transpose of the input matrix |\n",
    "| *b* | Constant bias parameter |\n",
    "| *X* | some input |\n",
    "| *z* | the linear relationship between the output and input where *z* = *wX<sup>i</sup>+b* |\n",
    "| *a* | activation function |\n",
    "| *g*(*z*) | activation function where *a* = *g*(*z*) |\n",
    "| *y<sup>i</sup>* | output to corresponding input *X<sup>i</sup>* |\n",
    "| *X<sup>i</sup>* | Each column in the input |\n",
    "|*p<sub>j</sub><sup>l</sup>* | Parameter where *l* is the parameter's layer and *j* is the parameter's index in the layer |\n",
    "| *i* | layer number |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
